# -*- coding: utf-8 -*-
"""recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pmG4rt-7yldUgZg0lTLw-0M8F_8tlO9e
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

_=! rm -rf ./logs/

"""## **Load Data**"""

!pip install numpy
!pip install keras
!pip install tensorflow

# import library
import os
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
from keras.callbacks import Callback, ModelCheckpoint, CSVLogger, TensorBoard

tf.get_logger().setLevel('ERROR')

# print tensorflow version
tf.__version__

# get data from github
!wget https://raw.githubusercontent.com/Tresure-Bangkit2023/ml-tresure/main/data/user.csv
!wget https://raw.githubusercontent.com/Tresure-Bangkit2023/ml-tresure/main/data/tourism_with_id.csv
!wget https://raw.githubusercontent.com/Tresure-Bangkit2023/ml-tresure/main/data/tourism_rating.csv
!wget https://raw.githubusercontent.com/Tresure-Bangkit2023/ml-tresure/main/data/package_tourism.csv

"""## **EDA**"""

#
# Read CSV and save as Pandas DataFrame
#
df_user             = pd.read_csv('user.csv')
df_tourism_with_id  = pd.read_csv('tourism_with_id.csv')
df_tourism_rating   = pd.read_csv('tourism_rating.csv')
df_package_tourism  = pd.read_csv('package_tourism.csv')

# print dataframes' shape
print(df_user.shape, df_tourism_with_id.shape,
      df_tourism_rating.shape, df_package_tourism.shape)

# print each columns in dataframe
print("{}\n{}\n{}\n{}".format(df_user.columns, df_tourism_with_id.columns,
                              df_tourism_rating.columns, df_package_tourism.columns))

"""To build this model, we only need User_Id, Place_Id, and Rating. So we will only focused on this three variables."""

# Make place lookup so we can only see the place's id and its category
df_lookup = df_tourism_with_id.copy()
df_lookup = df_lookup.drop(['Description', 'City', 'Price', 'Rating',
              'Time_Minutes', 'Coordinate', 'Lat', 'Long', 'Unnamed: 11',
              'Unnamed: 12'], axis=1)

df_lookup

# Drop rating under 3
df = df_tourism_rating.copy()
df = df[df['Place_Ratings'] >= 3]
df['Place_Ratings'].value_counts()

# see the df shape after dropping
df.shape

"""After dropping rating <3, the dataset's shape is (6223, 3) from (10000, 3). This mean there're 3777 records we drop this time"""

# Check the difference of unique value's count in dataset
print("==BEFORE DROPPING===")
print("Users: {}\nPlaces: {}".format(df_tourism_rating['User_Id'].nunique(),
 (df_tourism_rating['Place_Id']).nunique()))
print("\n==AFTER DROPPING===")
print("Users: {}\nPlaces: {}".format(df['User_Id'].nunique(),
 (df['Place_Id']).nunique()))

"""As we see, the unique values are same, we only dropped places with bad ratings from the users."""

# Check the Null and NaN values
if df.isna().any().any():
    print("Null values detected: {}".format(df.isna().sum().sum()))
else: print("There's no NaN values detected!")

print("\nTo check the null values, we can inspect Non-Null count below:")
df.info()

"""That's it! we already have a clean data. Our dataset only contains User_ID, Place_Id, and Place_Ratings, so there's no need to check the outliers.

## **PREP**
"""

#  make function to split train data and test data
def df_split(df):
  # make a copy from df
  df_train = pd.DataFrame()
  df_test = pd.DataFrame()
  groups = df.groupby('User_Id')

  for name, group in groups:
    n = group.shape[0]

    # calculate 20% of row counts
    n_test = int(n * 0.2)
    test_rows = group.sample(n=n_test)

    # data train
    train_rows = group.drop(test_rows.index)

    df_test = pd.concat([df_test, test_rows])
    df_train = pd.concat([df_train, train_rows])

  return df_train, df_test

# split dataframe
df_train, df_test = df_split(df)

df_test.shape

# make a matrix where
# rows for users and cols for places
rows_train = df_train.User_Id.astype(int)
cols_train = df_train.Place_Id.astype(int)
rows_test = df_test.User_Id.astype(int)
cols_test = df_test.Place_Id.astype(int)

# values for matrix (ratings)
values_train = list(df_train.Place_Ratings)
values_test = list(df_test.Place_Ratings)

# make ids for users and places in train data and test data
uids_train = np.array(rows_train.tolist())
pids_train = np.array(cols_train.tolist())
uids_test = np.array(rows_test.tolist())
pids_test = np.array(cols_test.tolist())

# check and sort unique values in train and test data
print("=== Unique values in train data ===")
users_train = list(np.sort(df_train.User_Id.unique()))
places_train = list(np.sort(df_train.Place_Id.unique()))
print("user: {}\nplace:{}".format(len(users_train), len(places_train)))

print("\n=== Unique values in test data ===")
users_test = list(np.sort(df_test.User_Id.unique()))
places_test = list(np.sort(df_test.Place_Id.unique()))
print("user: {}\nplace:{}".format(len(users_test), len(places_test)))

# count the positive rating to determine how many dummy negative rating we should add
count_positive = df.groupby(['User_Id']).count()
print("Minimal positive ratings count:\n{}\n".format(count_positive.min()))
print("Positive ratings' mean count:\n{}\n".format(count_positive.mean()))
print("Maximal positive ratings count:\n{}\n".format(count_positive.max()))

"""From the output, we agree to take mean positive ratings for dummy negative ratings (21)"""

# function to generate labels
def get_data_instances(uids, pids, places):
  user_input, place_input, labels = [], [], []
  # merge list of uids and pids
  zipped = set(zip(uids, pids))

  # place recommend to user
  for (u, p) in zip(uids, pids):
    user_input.append(u)
    place_input.append(p)
    labels.append(1)

    # places not recommend for user
    # adding 4 negative labels with assumption
    for t in range(21):
      j = np.random.randint(len(places))
      while (u, j) in zipped:
        j  = np.random.randint(len(places))
      user_input.append(u)
      place_input.append(j)
      labels.append(0)

  return user_input, place_input, labels

# TRAIN DATA
user_input_train, place_input_train, labels_train = get_data_instances(uids_train, pids_train, places_train)
print(len(user_input_train), len(place_input_train), len(labels_train))

# TEST DATA
user_input_test, place_input_test, labels_test = get_data_instances(uids_test, pids_test, places_test)
print(len(user_input_test), len(place_input_test), len(labels_test))

df_train['Place_Ratings'].value_counts()

"""##**CALLBACK**"""

class stopTrainingCallback(Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') >= 0.98 and logs.get('val_accuracy') >= 0.98):
      print('\n Your accuracy achieved 98%!')
      self.model.stop_training = True

log_csv = CSVLogger('model_result.csv', separator=";", append=False)

logdir="logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=logdir)

callback_list = [stopTrainingCallback(),
                 log_csv,
                 tensorboard_callback]

"""## **MODEL**"""

class RecommenderModel(tf.keras.Model):

  # Function iniziation
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderModel, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size

    self.user_embedding = tf.keras.layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l1(1e-8) # prevent overfitting
    )
    self.user_bias = tf.keras.layers.Embedding(num_users, 1) # layer embedding user bias
    self.place_embedding = tf.keras.layers.Embedding( # layer embeddings place
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l1(1e-8)
    )
    self.place_bias = tf.keras.layers.Embedding(num_place, 1) # layer embedding place bias

    self.dense1 = tf.keras.layers.Dense(512, activation='relu')
    self.batch_norm1 = tf.keras.layers.BatchNormalization(axis=-1)
    self.dropout1 = tf.keras.layers.Dropout(0.2) # dropping random values to prevent overfitting

    self.dense2 = tf.keras.layers.Dense(256, activation='relu')
    self.batch_norm2 = tf.keras.layers.BatchNormalization(axis=-1)
    self.dropout2 = tf.keras.layers.Dropout(0.2)

    self.dense3 = tf.keras.layers.Dense(128, activation='relu')

    self.prediction = tf.keras.layers.Dense(1, activation='sigmoid')

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # call layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # call layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # call layer embedding 3
    place_bias = self.place_bias(inputs[:, 1]) # call layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias
    x = self.dense1(x)
    x = self.batch_norm1(x)
    x = self.dropout1(x)

    x = self.dense2(x)
    x = self.batch_norm2(x)
    x = self.dropout2(x)
    x= self.dense3(x)

    return self.prediction(x) # activation sigmoid

##############
users_num = df['User_Id'].nunique()
places_num = df['Place_Id'].nunique()

model = RecommenderModel(users_num+1, places_num+1, 128)

# model compile
model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001),
              metrics=['mse', 'accuracy'])
model.build(input_shape = (None, 2))
model.summary()

# transform datatype so it can processed by model
x_train = pd.DataFrame({
    'User_Id' : user_input_train,
    'Place_Id': place_input_train
})

y_train = labels_train

x_val = pd.DataFrame({
    'User_Id' : user_input_test,
    'Place_Id': place_input_test
})

y_val = labels_test
x_train = x_train.values
y_train = np.array(y_train)
x_val = x_val.values
y_val = np.array(y_val)

history = model.fit(
    x_train,
    y_train,
    verbose = 1,
    batch_size = 128,
    epochs = 50,
    validation_data = (x_val, y_val),
    callbacks = callback_list
)

"""After so many trials and errors for the value of n, the optimumm value found so far is 21. This value give us the most stable validation loss compared to other values of n. But because it has more data, the downside is the time to finish one epoch is longer."""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

loss, mse, acc = model.evaluate(x_val, y_val)
print("Accuracy = ", (acc * 100.0), "%")

# plot the loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.show
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')

model.save_weights('/content/drive/MyDrive/saved_model/recommendationmodel.hdf5')